{"cells":[{"cell_type":"markdown","metadata":{"id":"FPKdecJfx6nr"},"source":["# SimpleGPT\n","### HW3 @ DL Course, Dr. Soleymani\n","\n","*Full Name:* Mehdi Jamalkhah\n","\n","*SID:* ...\n","\n","The objective of this notebook is to create and train a decoder-only model, which is a custom and scaled-down version of GPT, using the specified dataset.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TmDJ5RdJpJX3"},"source":["### import libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dfztaXelrAjB"},"outputs":[],"source":["# Import necessary libraries for data manipulation\n","import pandas as pd\n","import numpy as np\n","\n","# Import PyTorch and submodules for neural network construction and operations\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"markdown","metadata":{"id":"74cxkR23o-Rq"},"source":["### Download dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":490,"status":"ok","timestamp":1711617763796,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"xEXE-Xszqw46","outputId":"2a20b59a-241b-422f-96da-024617941af7"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-03-28 09:22:43--  https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5383844 (5.1M) [text/plain]\n","Saving to: ‘friends.csv’\n","\n","friends.csv         100%[===================>]   5.13M  --.-KB/s    in 0.05s   \n","\n","2024-03-28 09:22:43 (97.1 MB/s) - ‘friends.csv’ saved [5383844/5383844]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends.csv"]},{"cell_type":"markdown","metadata":{"id":"PvxCSyzlpbfs"},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":107,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1711617763797,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"rlT1e6J8NxDd","outputId":"bba4a3d6-6731-4eba-c332-3d9a4feed2db"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x79aed4322530>"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["batch_size = 16\n","block_size = 32  # Length of sequence fed into the model\n","max_iters = 5000  # Maximum number of training iterations\n","eval_interval = 100  # Interval for evaluating the model on validation data\n","learning_rate = 1e-3\n","\n","n_embd = 64  # Dimensionality of the embeddings\n","n_head = 4   # Number of attention heads\n","n_layer = 4  # Number of transformer layers\n","\n","eval_iters = 200  # Number of iterations to run during evaluation\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","torch.manual_seed(1337)\n"]},{"cell_type":"markdown","metadata":{"id":"ciQtkU4spQIA"},"source":["## Preparing dateset"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1711618164866,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"LnC75YdYIFVv","outputId":"9bb47042-9afd-4d3e-cc72-7c36b3f8b7e5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>speaker</th>\n","      <th>season</th>\n","      <th>episode</th>\n","      <th>scene</th>\n","      <th>utterance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>There's nothing to tell! He's just some guy I ...</td>\n","      <td>Monica Geller</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>C'mon, you're going out with the guy! There's ...</td>\n","      <td>Joey Tribbiani</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>All right Joey, be nice. So does he have a hum...</td>\n","      <td>Chandler Bing</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Wait, does he eat chalk?</td>\n","      <td>Phoebe Buffay</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(They all stare, bemused.)</td>\n","      <td>Scene Directions</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text           speaker  \\\n","0  There's nothing to tell! He's just some guy I ...     Monica Geller   \n","1  C'mon, you're going out with the guy! There's ...    Joey Tribbiani   \n","2  All right Joey, be nice. So does he have a hum...     Chandler Bing   \n","3                           Wait, does he eat chalk?     Phoebe Buffay   \n","4                         (They all stare, bemused.)  Scene Directions   \n","\n","   season  episode  scene  utterance  \n","0       1        1      1          1  \n","1       1        1      1          2  \n","2       1        1      1          3  \n","3       1        1      1          4  \n","4       1        1      1          5  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["friends_df = pd.read_csv('data/friends.csv')\n","friends_df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1711618165278,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"k-f3eh9aJ-lI","outputId":"798e5003-5df9-4f6f-c756-e9021bdd4afc"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>speaker</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>There's nothing to tell! He's just some guy I ...</td>\n","      <td>Monica</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>C'mon, you're going out with the guy! There's ...</td>\n","      <td>Joey</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>All right Joey, be nice. So does he have a hum...</td>\n","      <td>Chandler</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Wait, does he eat chalk?</td>\n","      <td>Phoebe</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Just, 'cause, I don't want her to go through w...</td>\n","      <td>Phoebe</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text   speaker\n","0  There's nothing to tell! He's just some guy I ...    Monica\n","1  C'mon, you're going out with the guy! There's ...      Joey\n","2  All right Joey, be nice. So does he have a hum...  Chandler\n","3                           Wait, does he eat chalk?    Phoebe\n","5  Just, 'cause, I don't want her to go through w...    Phoebe"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["friends_df = friends_df.drop(['episode','season','scene','utterance'], axis='columns')\n","friends_df = friends_df[friends_df['speaker'].str.contains('Scene')==False].copy()\n","friends_df['speaker'] = friends_df['speaker'].apply(lambda sp: sp.lower().capitalize().split(' ')[0])\n","\n","friends_df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2422,"status":"ok","timestamp":1711618177693,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"xP9TzA7BqwlZ","outputId":"bd4acbc2-6274-4026-cec0-2f1fa2d65aac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of dataset in characters: 3774765\n"]}],"source":["# Generate the dataset text\n","text = '\\n\\n'.join(f\"{row['speaker']}:\\n{row['text']}\" for _, row in friends_df.iterrows())\n","print(\"Length of dataset in characters:\", len(text))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1711617771227,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"2c5V0FvqseE0","outputId":"a9786068-7265-416d-bb17-42804a4e2850"},"outputs":[{"name":"stdout","output_type":"stream","text":["Monica:\n","There's nothing to tell! He's just some guy I work with!\n","\n","Joey:\n","C'mon, you're going out with the guy! There's gotta be something wrong with him!\n","\n","Chandler:\n","All right Joey, be nice. So does he have a hump? A hump and a hairpiece?\n","\n","Phoebe:\n","Wait, does he eat chalk?\n","\n","Phoebe:\n","Just, 'cause, I don't want her to go through what I went through with Carl- oh!\n","\n","Monica:\n","Okay, everybody relax. This is not even a date. It's just two people going out to dinner and- not having sex.\n","\n","Chandler:\n","Sounds like a date to me.\n","\n","Chandler:\n","Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.\n","\n","#all#:\n","Oh, yeah. Had that dream.\n","\n","Chandler:\n","Then I look down, and I realize there's a phone... there.\n","\n","Joey:\n","Instead of...?\n","\n","Chandler:\n","That's right.\n","\n","Joey:\n","Never had that dream.\n","\n","Phoebe:\n","No.\n","\n","Chandler:\n","All of a sudden, the phone starts to ring. Now I don't know what to do, everybody starts looking at me.\n","\n","Monica:\n","And they weren't looking at you before?!\n","\n"]}],"source":["# Print the first 1000 characters of the dataset text\n","print(text[:1000])"]},{"cell_type":"code","execution_count":98,"metadata":{"id":"oLYd4qOGN04D"},"outputs":[],"source":["# Create a vocabulary and encode/decode functions\n","chars = sorted(set(text))\n","vocab_size = len(chars)\n","char_to_id = {ch: i for i, ch in enumerate(chars)}\n","id_to_char = {i: ch for i, ch in enumerate(chars)}\n","\n","def encode(string):\n","    return [char_to_id[char] for char in string]\n","\n","def decode(ids):\n","    return ''.join(id_to_char[id] for id in ids)"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1529,"status":"ok","timestamp":1711617772749,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"-X_Pb0b7N225","outputId":"2799a66a-57b2-4647-a971-75a343f872a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary Size: 88\n","Training Data Length: 3397288\n","Validation Data Length: 377477\n"]}],"source":["# Prepare the data for model training\n","data = torch.LongTensor(encode(text))\n","train_part = int(0.9 * len(data))\n","train_data, val_data = data[:train_part], data[train_part:]\n","\n","\n","# Display information about the prepared data\n","print(f\"Vocabulary Size: {vocab_size}\")\n","print(f\"Training Data Length: {len(train_data)}\")\n","print(f\"Validation Data Length: {len(val_data)}\")"]},{"cell_type":"markdown","metadata":{"id":"n4FsOCySpw95"},"source":["## Utils"]},{"cell_type":"code","execution_count":115,"metadata":{"id":"Wu21-_C1KPjM"},"outputs":[],"source":["def get_random_batch(data_source, block_size, batch_size):\n","    \"\"\"\n","    Generates a random batch of input and label tensors from the data source.\n","\n","    Parameters:\n","    - data_source: The dataset from which to sample.\n","    - block_size: The size of each sequence to be sampled.\n","    - batch_size: The number of sequences per batch.\n","\n","    Returns:\n","    - A tuple of input and label tensors for the batch.\n","    \"\"\"\n","    indices = torch.randint(high=len(data_source) - block_size, size=(batch_size,))\n","    inputs = torch.stack([data_source[idx: idx + block_size] for idx in indices]).to(device)\n","    labels = torch.stack([data_source[idx + 1: idx + block_size + 1] for idx in indices]).to(device)\n","    return inputs, labels\n","\n","\n","def estimate_loss(model, data_sources, block_size, batch_size, eval_iters):\n","    \"\"\"\n","    Estimates the model's loss on different data splits.\n","\n","    Parameters:\n","    - model: The model to evaluate.\n","    - data_sources: A dictionary of datasets for each split.\n","    - block_size: The size of each sequence block.\n","    - batch_size: The number of sequences per batch.\n","    - eval_iters: The number of iterations for evaluation.\n","\n","\n","    Returns:\n","    - A dictionary with the mean loss for each data split.\n","    \"\"\"\n","    losses_dict = {}\n","    model.eval()\n","    with torch.no_grad():\n","        for split, data_source in data_sources.items():\n","            losses = [model(*get_random_batch(data_source, block_size, batch_size))[1].item() for _ in range(eval_iters)]\n","            losses_dict[split] = torch.tensor(losses).mean()\n","    model.train()\n","    return losses_dict\n","\n","def generate_text(model, initial_idx, block_size, max_new_tokens):\n","    \"\"\"\n","    Generates text by sampling from the model's predictions.\n","\n","    Parameters:\n","    - model: The model to use for text generation.\n","    - initial_idx: The initial indices for generation.\n","    - block_size: The size of the block to consider for each prediction.\n","    - max_new_tokens: The maximum number of tokens to generate.\n","\n","\n","    Returns:\n","    - A tensor of indices representing the generated text.\n","    \"\"\"\n","    idx = initial_idx\n","    model.eval()\n","    with torch.no_grad():\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]\n","            logits, _ = model(idx_cond)\n","            probs = F.softmax(logits[:, -1, :], dim=-1)\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, idx_next), dim=1)\n","    model.train()\n","    return idx\n","\n","\n","def train_model(model, train_data, val_data, block_size, batch_size, max_iters, eval_interval, optimizer):\n","    \"\"\"\n","    Trains the model on the training data and evaluates it on the validation data.\n","\n","    Parameters:\n","    - model: The model to train.\n","    - train_data: The training dataset.\n","    - val_data: The validation dataset.\n","    - block_size: The size of each sequence block.\n","    - batch_size: The number of sequences per batch.\n","    - max_iters: The maximum number of iterations for training.\n","    - eval_interval: The interval at which to evaluate the model.\n","    - optimizer: The optimizer for training the model.\n","\n","    Returns:\n","    - The trained model.\n","    \"\"\"\n","    data_sources = {'train': train_data, 'val': val_data}\n","    for iteration in range(max_iters):\n","        if iteration % eval_interval == 0 or iteration == max_iters - 1:\n","            losses = estimate_loss(model, data_sources, block_size, batch_size, eval_iters)\n","            print(f\"Iteration {iteration}: Train Loss {losses['train']:.4f}, Val Loss {losses['val']:.4f}\")\n","\n","        inputs, labels = get_random_batch(train_data, block_size, batch_size)\n","        optimizer.zero_grad()\n","        _, loss = model(inputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    return model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tWnjEfO6p6rr"},"source":["# Model architecture"]},{"cell_type":"markdown","metadata":{"id":"6Im5cAsEEFIY"},"source":["The Generative Pre-trained Transformer (GPT) model represents a significant breakthrough in the field of natural language processing (NLP) and beyond, thanks to its ability to generate human-like text based on the input it receives. Its architecture is based on the Transformer model, which allows it to effectively capture the context and semantics of the input text over long distances, making it particularly adept at tasks such as language modeling, text generation, and even complex reasoning tasks.\n","\n","Here's a brief overview of the decoder-only architecture(like GPT) and steps you can follow to implement its components:\n","\n","## 1. Understanding the Transformer Block\n","\n","The core of the decoder-only architecture is the Transformer block, which consists of two main components: multi-head self-attention and position-wise feed-forward networks. Each block applies these components in sequence, each followed by layer normalization and a residual connection.\n","\n","\n","*   **Multi-Head Self-Attention:** This mechanism allows the model to weigh the importance of different words in the input sequence differently, providing a dynamic way to aggregate context from the entire sequence.\n","\n","![MHSA](https://miro.medium.com/v2/resize:fit:720/format:webp/1*PiZyU-_J_nWixsTjXOUP7Q.png)\n","\n","*   **Position-wise Feed-Forward Networks:** These are simple, fully connected neural networks applied to each position separately and identically. This means they look at each word (or token) in isolation and then transform it.\n","\n","## 2. Understanding the whole architecture\n","To build a decode-only architecture, you would generally follow these steps:\n","\n","\n","\n","*   **Embedding Layer:** This is where the model learns representations for each token in the vocabulary and for each possible position in the input sequence. The embeddings for tokens and their positions are summed to produce a single representation for each token that captures both its meaning and its position in the sequence.\n","\n","*   **Stack of Transformer Blocks:** The heart of the model. Several Transformer blocks are stacked on top of each other to allow the model to learn complex relationships between tokens in the input sequence. Each block includes multi-head self-attention and feed-forward networks, as explained above.\n","\n","*   **Output Layer:** After passing through the Transformer blocks, the output is normalized and then passed through a linear layer that projects it back to the size of the vocabulary. This produces a set of logits that can be used, with a softmax layer, to generate probabilities for each token in the vocabulary being the next token in the sequence.\n","\n","![](https://miro.medium.com/v2/resize:fit:700/0*77memcl1VYIdpE8f.png)\n","\n","\n","\n","\n","\n","\n","---\n","Now for implementing SimpleGPT model you should code the components described above. Here's a approach to doing so:\n","\n","\n","1.   **SelfAttentionHead:** Implement the self-attention mechanism with key, query, and value projections. Don't forget to apply masking to ignore future tokens in the sequence when calculating attention scores.\n","2.   **MultiHeadSelfAttention:** Aggregate multiple self-attention heads, allowing the model to focus on different parts of the input sequence simultaneously.\n","3.   **FeedForward:** Implement the position-wise feed-forward network with a simple sequence of linear layers and activation functions.\n","4.   **TransformerBlock:** Combine the multi-head self-attention and feed-forward network, adding normalization and residual connections around each.\n","5.   **SimpleGPT:** Assemble the model by starting with embedding layers for tokens and positions, stacking several Transformer blocks, and then adding the output layer to produce logits.\n"]},{"cell_type":"markdown","metadata":{"id":"EzHTTdxvqH1s"},"source":["## Transformer block"]},{"cell_type":"code","execution_count":101,"metadata":{"id":"87TGFQqFN-g2"},"outputs":[],"source":["class SelfAttentionHead(nn.Module):\n","    \"\"\"\n","    Implements a single head of self-attention.\n","\n","    This module applies self-attention on the input data, allowing the model to weigh the importance of different tokens within the same input sequence.\n","\n","    Args:\n","        n_embd (int): Dimensionality of the embeddings.\n","        head_size (int): Size of each attention head.\n","\n","    Attributes:\n","        key, query, value (nn.Linear): Linear transformations for computing self-attention mechanism's components.\n","    \"\"\"\n","\n","    def __init__(self, n_embd, head_size):\n","        super().__init__()\n","        \n","        self.head_size = head_size\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass for self-attention head.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor (batch_size, seq_length, n_embd).\n","\n","        Returns:\n","            torch.Tensor: Output tensor after applying self-attention.\n","        \"\"\"\n","        # Make sure to create a mask and use it on the attention weights.\n","        # You can do this by using torch.tril to make a lower triangle mask and masked_fill_ in PyTorch to put the mask in place\n","        K = self.key(x)\n","        Q = self.query(x)\n","        V = self.value(x)\n","        alignment = torch.bmm(Q, K.permute(0, 2, 1)) / np.sqrt(self.head_size)\n","        alignment = torch.masked_fill(alignment, torch.tril(alignment) == 0, -torch.inf)\n","        attention = torch.softmax(alignment, dim=-1)\n","        out = torch.bmm(attention, V)\n","        \n","        return out\n","\n","\n","class MultiHeadSelfAttention(nn.Module):\n","    \"\"\"\n","    Implements multi-head self-attention by running several self-attention mechanisms in parallel.\n","\n","    Args:\n","        num_heads (int): Number of attention heads.\n","        input_size (int): Size of each input token.\n","        head_size (int): Size of each attention head.\n","\n","    Attributes:\n","        heads (nn.ModuleList): ModuleList containing all the self-attention heads.\n","        projection (nn.Linear): Linear layer to project the concatenated outputs of all heads back to the input_size dimensions.\n","    \"\"\"\n","\n","    def __init__(self, num_heads, n_embd, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([SelfAttentionHead(n_embd, head_size) for i in range(num_heads)])\n","        self.projection = nn.Linear(head_size*num_heads, n_embd)\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass for multi-head self-attention.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor (batch_size, seq_length, input_size).\n","\n","        Returns:\n","            torch.Tensor: Output tensor after applying multi-head self-attention.\n","        \"\"\"\n","        out = torch.concatenate([head(x) for head in self.heads], dim=-1)\n","        out = self.projection(out)\n","        return out\n","\n","\n","class FeedForward(nn.Module):\n","    \"\"\"\n","    Implements a simple feed-forward neural network as part of the transformer block.\n","\n","    Args:\n","        n_embd (int): Dimensionality of the embeddings.\n","\n","    Attributes:\n","        net (nn.Sequential): A sequence of linear layers and a ReLU activation function.\n","    \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(nn.Linear(n_embd, 4 * n_embd), \n","                                 nn.ReLU(),\n","                                 nn.Linear(4 * n_embd, n_embd)) \n","    def forward(self, x):\n","        \"\"\"Perform forward pass through the feedforward layer.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor after feedforward computation.\n","\n","        \"\"\"\n","        output = self.net(x)\n","        return output"]},{"cell_type":"code","execution_count":102,"metadata":{"id":"luJq9Za7osBU"},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","    \"\"\"\n","    Implements a Transformer block with self-attention and feed-forward layers.\n","\n","    This class combines multi-head self-attention and a position-wise feed-forward network,\n","    each followed by layer normalization and residual connections.\n","\n","    Args:\n","        n_embd (int): Dimensionality of the embeddings.\n","        num_heads (int): Number of heads in the multi-head self-attention component.\n","\n","    Attributes:\n","        self_attention (MultiHeadSelfAttention): The multi-head self-attention module.\n","        feed_forward (FeedForward): The feed-forward neural network module.\n","        norm1, norm2 (nn.LayerNorm): Layer normalization modules.\n","    \"\"\"\n","\n","    def __init__(self, n_embd, num_heads):\n","        super().__init__()\n","        self.self_attention = MultiHeadSelfAttention(num_heads, n_embd, n_embd // num_heads)\n","        self.feed_forward = FeedForward(n_embd)\n","        self.norm1 = nn.LayerNorm(n_embd)\n","        self.norm2 = nn.LayerNorm(n_embd)\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the Transformer block.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_size).\n","\n","        Returns:\n","            torch.Tensor: Output tensor of the same shape as input.\n","        \"\"\"\n","        x1 = self.self_attention(x)\n","        x2 = self.norm1(x1 + x)\n","        x3 = self.feed_forward(x2)\n","        x4 = self.norm2(x3 + x2)\n","        return x4"]},{"cell_type":"markdown","metadata":{"id":"5FjyRYGsqQ8h"},"source":["## Model"]},{"cell_type":"code","execution_count":119,"metadata":{"id":"hoelkOrFY8bN"},"outputs":[],"source":["class SimpleGPT(nn.Module):\n","    \"\"\"SimpleGPT model for sequence generation tasks.\n","\n","    This model consists of an embedding layer for tokens and positions, followed by a stack of transformer blocks.\n","    It then applies layer normalization and a linear layer to generate logits for the vocabulary.\n","\n","    Args:\n","        vocab_size (int): Size of the vocabulary.\n","        n_embd (int): Dimensionality of the token embeddings and hidden layers.\n","        block_size (int): Size of the input sequence block.\n","        n_layer (int): Number of transformer blocks.\n","        n_head (int): Number of attention heads.\n","\n","    Attributes:\n","        token_embeddings (nn.Embedding): Embedding layer for tokens.\n","        position_embeddings (nn.Embedding): Embedding layer for positions.\n","        blocks (nn.Sequential): Sequential module containing transformer blocks.\n","        layer_norm (nn.LayerNorm): Layer normalization module.\n","        lm_head (nn.Linear): Linear layer for generating logits.\n","\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, n_embd, block_size, n_layer, n_head):\n","        super().__init__()\n","        self.token_embeddings = nn.Embedding(vocab_size, n_embd)\n","        self.position_embeddings = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head) for i in range(n_layer)]) \n","        self.layer_norm = nn.LayerNorm(n_embd)\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        \"\"\"Perform forward pass through the SimpleGPT model.\n","\n","        Args:\n","            idx (torch.Tensor): Input tensor containing token indices.\n","            targets (torch.Tensor, optional): Target tensor containing token indices for computing the loss.\n","\n","        Returns:\n","            tuple: Tuple containing logits tensor and optional loss tensor.\n","\n","        \"\"\"\n","        # hint: token_emb = self.token_embeddings(inputs) + self.position_embeddings(torch.arange(inputs_sequence_length))\n","\n","        token_emb = self.token_embeddings(idx) + self.position_embeddings(torch.arange(idx.shape[-1]))\n","        out = self.blocks(token_emb)\n","        out = self.layer_norm(out)\n","        logits = self.lm_head(out)\n","\n","\n","\n","        loss = None\n","        if targets is not None:\n","            B, T, E = logits.shape\n","            probs = logits[torch.arange(B).repeat_interleave(T), torch.arange(T).repeat(B), targets.flatten()].view(B, -1)\n","            loss = -probs.softmax(dim=1).log().sum() / (B * T)\n","        \n","        return logits, loss\n"]},{"cell_type":"code","execution_count":128,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1711617772752,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"YbS3pJH0RL65","outputId":"5fa1069a-bbc4-4205-9ec5-2934c2cc6ef1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters = 212696\n"]}],"source":["# Initialize the model and move it to the appropriate device\n","model = SimpleGPT(vocab_size=vocab_size, n_embd=n_embd, block_size=block_size, n_layer=n_layer, n_head=n_head).to(device)\n","\n","# Calculate the number of parameters in the model\n","num_parameters = sum(p.numel() for p in model.parameters())\n","print(f'Number of parameters = {num_parameters}')"]},{"cell_type":"code","execution_count":126,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1711617772752,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"45LGegT3W86c","outputId":"db1ff6ea-20a8-4dcf-a609-159ecb8921f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["SimpleGPT(\n","  (token_embeddings): Embedding(88, 64)\n","  (position_embeddings): Embedding(32, 64)\n","  (blocks): Sequential(\n","    (0): TransformerBlock(\n","      (self_attention): MultiHeadSelfAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x SelfAttentionHead(\n","            (key): Linear(in_features=64, out_features=16, bias=False)\n","            (query): Linear(in_features=64, out_features=16, bias=False)\n","            (value): Linear(in_features=64, out_features=16, bias=False)\n","          )\n","        )\n","        (projection): Linear(in_features=64, out_features=64, bias=True)\n","      )\n","      (feed_forward): FeedForward(\n","        (net): Sequential(\n","          (0): Linear(in_features=64, out_features=256, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=256, out_features=64, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (1): TransformerBlock(\n","      (self_attention): MultiHeadSelfAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x SelfAttentionHead(\n","            (key): Linear(in_features=64, out_features=16, bias=False)\n","            (query): Linear(in_features=64, out_features=16, bias=False)\n","            (value): Linear(in_features=64, out_features=16, bias=False)\n","          )\n","        )\n","        (projection): Linear(in_features=64, out_features=64, bias=True)\n","      )\n","      (feed_forward): FeedForward(\n","        (net): Sequential(\n","          (0): Linear(in_features=64, out_features=256, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=256, out_features=64, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (2): TransformerBlock(\n","      (self_attention): MultiHeadSelfAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x SelfAttentionHead(\n","            (key): Linear(in_features=64, out_features=16, bias=False)\n","            (query): Linear(in_features=64, out_features=16, bias=False)\n","            (value): Linear(in_features=64, out_features=16, bias=False)\n","          )\n","        )\n","        (projection): Linear(in_features=64, out_features=64, bias=True)\n","      )\n","      (feed_forward): FeedForward(\n","        (net): Sequential(\n","          (0): Linear(in_features=64, out_features=256, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=256, out_features=64, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (3): TransformerBlock(\n","      (self_attention): MultiHeadSelfAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x SelfAttentionHead(\n","            (key): Linear(in_features=64, out_features=16, bias=False)\n","            (query): Linear(in_features=64, out_features=16, bias=False)\n","            (value): Linear(in_features=64, out_features=16, bias=False)\n","          )\n","        )\n","        (projection): Linear(in_features=64, out_features=64, bias=True)\n","      )\n","      (feed_forward): FeedForward(\n","        (net): Sequential(\n","          (0): Linear(in_features=64, out_features=256, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=256, out_features=64, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","  (lm_head): Linear(in_features=64, out_features=88, bias=True)\n",")\n"]}],"source":["# Print the model structure\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"GE4lP5yNqY62"},"source":["# training and evaluation the model"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def generate_text(model, initial_idx, block_size, max_new_tokens):\n","    \"\"\"\n","    Generates text by sampling from the model's predictions.\n","\n","    Parameters:\n","    - model: The model to use for text generation.\n","    - initial_idx: The initial indices for generation.\n","    - block_size: The size of the block to consider for each prediction.\n","    - max_new_tokens: The maximum number of tokens to generate.\n","\n","\n","    Returns:\n","    - A tensor of indices representing the generated text.\n","    \"\"\"\n","    idx = initial_idx\n","    model.eval()\n","    with torch.no_grad():\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]\n","            logits, _ = model(idx_cond)\n","            probs = F.softmax(logits[:, -1, :], dim=-1)\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, idx_next), dim=1)\n","    model.train()\n","    return idx"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9632,"status":"ok","timestamp":1711617782376,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"8Ij6iV37ROZo","outputId":"1980b6b7-e238-4c31-8320-622c240b5734"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Y1sx*stN1gh>vQt*2%fD%ADJihc7}O_,sQNvNi'2'Bj$j)`Rj)>UyMNlsRUN\n","KVIhSvksLmDjv\"B}h81e\n","Gd!\"GGyh>68R>:ddBYd}(Jwl%Qy9%v!mf\"eH>-&>U5vG\"S/2`dpOwd;{!K$exSegc>QV5_nB}hDqUM59B!6tjEVjb8L/IZ0wuh}{cl?(-#sd5sbI6KS?dM5[Z-d'w,*XdH*xXM8,6e\"13M\"ve6K%M%p{FjaO>&/1teS/GM\"6uN(jdad'SWg'vBMM,{\"uLS !(ApNf&pp*kVP}M-w)dH%>e).QGJ9dN{R\"E9`h(.BxV7R(M*Mx4IFg%p,M1 >)_IB&+udF-k,}`dpFSy -yR%r-*$AUySFr%r#UCifB8P>9hB6-6v>)Mz#6(Mmyv#)Y}:j/c'$SPV`BHGF}'j31#lRp*7nXmfcnlH9B:aNH+ [ALa!LNM74zQ&AOOnRV)*hJwy/0QH_7/[M:FN>`-nrBIq6U{V.y(LhM>j*zZzqhQ#ByM2ydv,N$pcAp:M8}*>{zBxjx*Mi5)`dVJ.TpMBq`[ {(IB4h)fmEchdNd66ur7(%lQB}RbEGKMh&>*30#1j#K\"+cGl&Z-dRwc)%Yry:2-AB yfXweAU[Cy6nx!,NtfJ0JwjnKy.A#FwG-{FBV>\n","`ed!ledME)}%+s4>ZBQFdM7}?8jX/8:4uBikUj[FWx1jHnfV)KacWH/hWO&?GV:(#Be$Nd!ZN+\"s /_sgQKrryyO#.F6Xrd*fn H%Q5$duKZty*}pM[kdwP$Ow'3U/`!>9cYVM/E5>ZBpSC1?ddY%%EB(wel4f!4--CdMHuG/T!offdhQ,c'B/6jMev)zkp1Ah#huBfsIf5Gcq_Dv)w)MjDF(.M/Hwl-BHE)MG!dM0azH*mlIjM98IB5d9!i/9OwZmV/M[a5 fvrMh*#Jy(U$jnDv49d\"6Nu!:+c9GU/QbP$ `>/-kRW/+.{XOdd*1pGwnbMIM$J\n","wdZOOX9W%ne %Rn&dV\n","w6d%$#>dg#ykNT,m+(-(M\"b!O65*`a4`NBOL;(`QXofQGsy%#zh-wIUhQo52KQ{ufzKcGUuw))AhHd1u>/NUe\"Nrqj#60e9oM-sroA6)f$}hBwn#h_%aNQYW'Xdk0k}MCE)dM{*Yl_6)6J/G(EflB/M*B>:?>'l9!(8bd2TddGf7unds`//l*}9/Jmm6s5*q%/{%hq$Z6}A!MMxhP:WiFrpQpOn7MByMjb'6XbMXA6r,G,iBREx5hlK/wMazDb:WSM#}\"0}W%jBSXM jDsC_)r`'!Lj%>UfAUP}shBSh\"Arx7.\n","kO1UA[Gg!gkV`[9,},(wAFA/z',?B.*Bckbn6(wBTvGc{hi`J8Sy&`BfEQN9dJG6RM*vQu:8>plihGwC:(LqAM{V`)YM!.D$`QXeB H(eQ5W!{Kf&`V5M!6*#\n",";GT-{\n","p/wn'c(3db69fIm6R3l>,6VxpM-B%2GGG!y/;fP{jp8olM2j}ydni':BOMMSWs8wG([dE$'/2* (p}(a\"x52XF)(g}5`xBhQAjI8GBpCEH}BpODdHPE*{-%A,&D edVp*PGHL.B}W9J6&}7fchD}f`-6Q7fG}-\",F6e:I%*lLPHdybB+o}O)#7jE78SfDoDDa>7H:C?AdwB%5arLvvM)e(Bw`\n","gyMX#y`\"Dn#9G}Vg SS?MPe#hD)6,GY/CwGM5+#fk55(Rvssd''p$u,B05ed:MmxfvBjwAd56 Sh/hnS\"f?kJ'3c'!K\"$K&U`6'#,Oyd86M0*w4P,nWf-}j\"BlFQ[d!fPO0*{KInnX_xsd}XFF:vN8B5%+,%>5U_F#vFc>c4b%Km1`#[mRgKc f6)dwZ-eehNNn&u&Ie1w wlddfBHLMX)+x*Y,jf pvnxm_ty*0M7ZGOnD1($9BY%h-NyH&Xnz>)!RH2R{p%,Q1$\n","$moH>`%F}B,M4wg[d$(s#pH#>*1pu\"gd2BMhs'U-5-j6!Yth,R`??WQyNw%}(pG`bXw5\"?d%E%z%F\n"]}],"source":["# Example of generating output with the initial model (before training)\n","initial_idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n","generated_output = generate_text(model, initial_idx, block_size, max_new_tokens=2000)\n","decoded_output = decode(generated_output[0].tolist())\n","print(decoded_output)"]},{"cell_type":"code","execution_count":129,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334820,"status":"ok","timestamp":1711618117190,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"u1524iCrPffs","outputId":"c62261a7-bbde-4010-c5b7-6adbc6a9076b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 0: Train Loss 3.6199, Val Loss 3.6191\n","Iteration 100: Train Loss 3.4661, Val Loss 3.4661\n","Iteration 200: Train Loss 3.4660, Val Loss 3.4660\n","Iteration 300: Train Loss 3.4660, Val Loss 3.4658\n","Iteration 400: Train Loss 3.4659, Val Loss 3.4664\n","Iteration 500: Train Loss 3.4658, Val Loss 3.4658\n","Iteration 600: Train Loss 3.4658, Val Loss 3.4657\n","Iteration 700: Train Loss 3.4657, Val Loss 3.4657\n","Iteration 800: Train Loss 3.4657, Val Loss 3.4657\n","Iteration 900: Train Loss 3.4657, Val Loss 3.4657\n","Iteration 1000: Train Loss 3.4660, Val Loss 3.4659\n","Iteration 1100: Train Loss 3.4658, Val Loss 3.4658\n","Iteration 1200: Train Loss 3.4658, Val Loss 3.4658\n","Iteration 1300: Train Loss 3.4657, Val Loss 3.4657\n","Iteration 1400: Train Loss 3.4657, Val Loss 3.4657\n","Iteration 1500: Train Loss 3.4657, Val Loss 3.4657\n","Iteration 1600: Train Loss 3.4657, Val Loss 3.4657\n","Iteration 1700: Train Loss 3.4657, Val Loss 3.4657\n","Iteration 1800: Train Loss 3.4657, Val Loss 3.4657\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[129], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_model(model, train_data, val_data, block_size, batch_size, max_iters, eval_interval, optimizer)\n","Cell \u001b[0;32mIn[115], line 96\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_data, val_data, block_size, batch_size, max_iters, eval_interval, optimizer)\u001b[0m\n\u001b[1;32m     94\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     95\u001b[0m     _, loss \u001b[38;5;241m=\u001b[39m model(inputs, labels)\n\u001b[0;32m---> 96\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     97\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# training\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate * 10)\n","trained_model = train_model(model, train_data, val_data, block_size, batch_size, max_iters, eval_interval, optimizer)"]},{"cell_type":"code","execution_count":117,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6723,"status":"ok","timestamp":1711618123906,"user":{"displayName":"mehran sarmadi","userId":"05713412677971440691"},"user_tz":-210},"id":"GvjP74BOwFrZ","outputId":"df0bd023-df57-4ce0-ec8f-c866862e1cf8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Vxcmzj_kI-}6&[7zat3HQuiBW!.[mAM:tI>GkkPn;V?+,$)O_xX9Ygz7R>O{{v:B`/n5},ZO:i%%>,JJ>LX%v-o-;5h#6O&x&%/t}/. BN>>[j\n","NR`wY'M{lmML5H[T3,wgj&W/Z/?\"t:(s89.Fsdb;$:gF50xfeli\"\n","J SY-`eZ'Mo.,7&#[lU+>4mYLK?luq3Vk:DuX#AH3Y},q4,%)-!-%r[jQ7{0Y,C/9w!`yFvCZ,.P'e9P.+(!tAZ?>U.?GNO:.OxH#)IX>ARp+vZT2r56,P(> 1s!R>u!U,Mv>O*Oteo\n","N&3qV+LfoW/XYdrNVJ!uCw[H(qVLY$y'iIKGMxz4T#4i*Y!&U8/jt5Q`P7XEv{lvOukcsc{$R`G};WUsh[:\n","n;EQMV;N3Osh*9t.&2[6Bg[U}GywOHhP$$I{ijRx\"i9w[q>APCoySgx$E}Iqj&fQ)2Q4o/:?jHFU2k'fA!f?EgaPM&{a>tw/sNjJNE%x,1lxIeQRlH7XuE1v1OUmhApd[?* */XL&a*zs i8uP%>t#:je_Gg\"17ne uy(u1Q-jBy;_H2:Et\n","9u*:#2L\n","z.kb({VRi/87*1`39Ohl3q,[z`n1a6A9WHnSk$9dH{G}'{cHbn4k_A!rB:_yzlBAI'(%pPw968Z5gE?3CEI'8%a-{{}W0yh/O&4u_-?owJxg/m5J#+(0E(Gp>cR\"\n","1K:Cy3$$S{XihELw*{i&{32d>/D{IMa&`PkQsPeI_\"U[7tLb\"yolO>K8 eLk'hoS)m\">!2\"NjNz1o(c$HFFx1Fj[2rb/rswYLXL;$.X4cT}JmA\"bPQ33'QZh{m.1C3{Fk,b_7b.c& yhk_,!z{b7g{CRzfP4Q9:yY:Qy_0KO'VQ1IlhucrRa60(.d71tX{{y)x8G}UmFPr'OYXTcaW9x8CtbR(Z>`Yo!oY1sh[lLSUn$YkI>o)d.5t4LQrx}22UD'&(vZ0g$nRd4sZx/F(b\"*qC:fvn,[r#o!0pyrM\";$1{CB3+\"5:?a?M$8#9T3\"sHPN?q&-MO#y\n","O97l#lZhF6R'ukijmBzb2`z\n","n55)Bgvi;>H[v[rxGgSfNY*4>fFkB%/5ozLdH37's+I8z6n>T>X}KDTH&j*AjVFrK}41f;!6;3:W\"mv+1D\n","eFBngxzVB(?&O_X?9l-`8v+a>ES-gXX'RyVg. 4`2\n","OBoIxEueRG,\n","mEoH6ajneSm:4_3SO-S!Aa&_c(qi7>{*[b\n","*m{fMR5N,/!Im+2ay2BsR\n","Q${8>j[5.oZcB/GV5SvqIgwek9e?Te-(\"({5eSBV,c%6g$(F[.{CQ*d3ibp$p>\n","18b;N rJM&Tv%X0}XaY1GiejJwilzj6F8ZQnVhQR..(jpUvm,G6IgI\n","OEnb;5[+8F{A%fAE#u8O\"\"k0xRpM3e{oK\"Osm_Z[w6>5ma'f;>{Ob8,spAFbJ6XIZEo\n","E5_6vCYdW8*y[2[L9oLCb(moT*V(u+JJZS#;>T%A_eaUYMarW1ADp-IpIF!OwPahG'-FOM$!$/#n`bl>j6NlcG7W$E)Tt/ MBhw0XR*[AoHYzYpL\n","7ce`6\"'NM[axs#:QqFd1L>rg.`fu.E9:+(Pb34[$e?mf3pjiueNd&T NDl-*F1D,mjAtM{Vc5cdoue*0H%3a[--OLW9.62KqxUuuO[7A*Ol{N)nT\"?bPviIp[:p#iK}z(w?Czg;l(r`-I{(R;MD,7S->4uT j9d g9%sPnXko0fUaBv\"ZGK29OOq>tS%$\n","PA{[919topQi5FTqam!dx31K7#8G[G{+h4P4z#P3 F7Lxk7.V6z;c\"GbQMbG$3&?:MGr+5r(nhRU-Nz1;{LFH>RmXn-5gw*n9zuDE,e r+Z`Yw0N%\n","m`RZNiw/N8e X`R!OG_.qe;iA8+ScE+CiyD$894[i;7L9RmBXG5M#!58sZDo(?(t#%N DTS)->PMs,8!ebkM/3omx\n","Lxd*aq\n","cfnFBnmIzP [dNp5>1xGaR&'W5#rY/`[q>z%8+[c.Y::Q{(F%M!\n"]}],"source":["# Example of generating output with the trained model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","generated_output = generate_text(trained_model, initial_idx, block_size, max_new_tokens=2000)\n","decoded_output = decode(generated_output[0].tolist())\n","print(decoded_output)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
