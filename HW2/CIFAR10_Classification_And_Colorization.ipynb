{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/home/mehdi/elearn/DL/HWs/HW2/vars/40.PDF']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "gdown.download_folder(\"https://drive.google.com/drive/folders/1GVb24Y9mzAIMVccSqHQOVQSIoV7ufsWR?usp=sharing\", quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def load(filename):\n",
        "    return torch.load('vars/' + filename + '.pt', map_location=torch.device('cpu'))\n",
        "\n",
        "def save(object, filename):\n",
        "    filename = 'vars/' + filename + '.pt'\n",
        "    if isinstance(object, torch.nn.Module):\n",
        "        torch.save(object.state_dict(), filename)\n",
        "    else:\n",
        "        torch.save(object, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAa6g3BcFCOi"
      },
      "source": [
        "# CIFAR10 Classification\n",
        "\n",
        "### HW2 @ DL Course, Dr. Soleymani\n",
        "\n",
        "*Full Name:* Mehdi Jamalkhah\n",
        "\n",
        "*SID:* ...\n",
        "\n",
        "In this part of the assignment we want to do an image classification task using PyTorch on CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej8Q1gQPbTyE"
      },
      "source": [
        "### Google Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8kheJRyVeFZ",
        "outputId": "18b1a9ea-faf2-4bed-d462-85e3a30e7f9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srq7CBcGVl18",
        "outputId": "2f923710-ae18-4e84-95b7-f0a03204c85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['data', 'CIFAR10_Classification_And_Colorization.ipynb', 'DL']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'DL/HW2'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "os.chdir(GOOGLE_DRIVE_PATH)\n",
        "print(os.listdir('./'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2GsrCzWTOIhI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIv1Mpsbvt8v"
      },
      "source": [
        "## Device\n",
        "\n",
        "Set device to work with (GPU or CPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2uGuIUtwSFAR",
        "outputId": "9eb4dcab-a0e7-4900-d49a-6b0cae061725"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CUDA = 'cuda'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4T4AL0cv1Jm"
      },
      "source": [
        "## Transforms & Dataset & Dataloader\n",
        "\n",
        "Here, you should download and load the dataset with the desire transforms. After that, you should split train dataset to train and validation sets. Finally, define the dataloaders for `train`, `validation` and `test`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4E6TT8whO9N4"
      },
      "outputs": [],
      "source": [
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yuo6wyCkEqMK"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FR0BpY0YO-Em"
      },
      "outputs": [],
      "source": [
        "# inverse the normilize transform to restore the original data\n",
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        tensor = tensor * self.std + self.mean\n",
        "        return tensor\n",
        "\n",
        "norminv = UnNormalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z3UuPXFQOSDX"
      },
      "outputs": [],
      "source": [
        "initial_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=False, transform=transform_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "61n36ZUkD-gA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "trainset, valset = random_split(initial_trainset, [45000, 5000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gXhlMgGkEBPs"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "godQEnIuEDRo",
        "outputId": "4120af6c-b97d-4326-9f0c-c62c7adbc19a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C-YjLZtwnq2"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Visualize 5 random images from each class in different columns\n",
        "\n",
        "- **Hint**:  You can use `plt.subplots` for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "sfwytktJVVCH",
        "outputId": "1099ef62-484a-496f-acf5-573c50e43563"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "indexes = np.random.randint(0 , len(images), 5)\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i, idx in enumerate(indexes):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(images[idx].permute(1, 2, 0))\n",
        "    plt.title(f\"Label: {classes[labels[idx]]}\")\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja-L0f1wybzU"
      },
      "source": [
        "## Model\n",
        "\n",
        "Define your ResNet model here from scratch (You are not allowed to use the existing models in pytorch)\n",
        "\n",
        "Our suggestion is to implement ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_ZtjLZl2VVCH"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ResNetBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, identity_downsample=None,stride=1):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "  \n",
        "    \n",
        "class ResNetLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1) -> None:\n",
        "        super(ResNetLayer, self).__init__()\n",
        "\n",
        "        identity_downsample = None\n",
        "        if stride != 1:\n",
        "            identity_downsample =  nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.block1 = ResNetBlock(in_channels, out_channels, identity_downsample, stride=stride)\n",
        "        self.block2 = ResNetBlock(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "class ResNet18(torch.nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = ResNetLayer(64, 64)\n",
        "        self.layer2 = ResNetLayer(64, 128, stride=2)\n",
        "        self.layer3 = ResNetLayer(128, 256, stride=2)\n",
        "        self.layer4 = ResNetLayer(256, 512, stride=2)\n",
        "\n",
        "        self.avgPool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgPool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYp77Euaz_5u"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odR5mfCA0Eqy"
      },
      "source": [
        "### Model instantiation\n",
        "\n",
        "Create an instance of your model and move it to `device`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "i3PjKY_oSBkg"
      },
      "outputs": [],
      "source": [
        "net = ResNet18().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "isinstance(torch.tensor([]), torch.nn.Module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8zn5eLs0bBS"
      },
      "source": [
        "### Criterion & Optimizater\n",
        "\n",
        "Define `criterion` and `optimizer` (Or `scheduler`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gEd5yXt3SL2T"
      },
      "outputs": [],
      "source": [
        "criterion =  nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.002, weight_decay=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gth9e1k70uAI"
      },
      "source": [
        "### Train loop\n",
        "\n",
        "Train your model\n",
        "\n",
        "Tasks:\n",
        "- [ ] Things that are needed to be printed in each epoch:\n",
        "  - Number of epoch\n",
        "  - Train loss\n",
        "  - Train accuracy\n",
        "  - Validation loss\n",
        "  - Validation accuracy\n",
        "- [ ] save train/validation loss and accuracy (of each epoch) in an array for later usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ONOcKjTxpDpQ"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y87a3RWjVVCJ"
      },
      "outputs": [],
      "source": [
        "def correct(output, target):\n",
        "    _, predicted = output.max(1)\n",
        "    return predicted.eq(target).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yFz0lVkUs8Mk"
      },
      "outputs": [],
      "source": [
        "def train_epoch(net: torch.nn.Module, criterion: torch.nn.Module, optimizer: torch.optim.Optimizer,scheduler: torch.optim.lr_scheduler ,dataloader: torch.utils.data.DataLoader):\n",
        "    net.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch, labels in dataloader:\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = net(batch)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total += labels.size(0)\n",
        "        corrects += correct(output, labels)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100 * corrects / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def eval_epoch(net: torch.nn.Module, criterion: torch.nn.Module, dataloader: torch.utils.data.DataLoader, test_mode: bool = False):\n",
        "    net.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    if test_mode:\n",
        "      incorrect_images = torch.tensor([]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, labels in dataloader:\n",
        "            batch, labels = batch.to(device), labels.to(device)\n",
        "\n",
        "            outputs = net(batch)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            corrects += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if test_mode:\n",
        "              incorrect_images = torch.concatenate([incorrect_images, batch[predicted.eq(labels)]])\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100.0 * corrects / total\n",
        "\n",
        "    if test_mode:\n",
        "      return avg_loss, accuracy, incorrect_images\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 8\n",
        "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]}\n",
        "\n",
        "if device == CUDA:\n",
        "    for e in range(epochs):\n",
        "        train_loss, train_acc = train_epoch(net, criterion, optimizer, scheduler, trainloader)\n",
        "        val_loss, val_acc = eval_epoch(net, criterion, valloader)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"(Epoch {e + 1} / {epochs}) train loss:{train_loss: .4f}; train acc:{train_acc: .2f}%; val loss:{val_loss: .4f}; val_acc:{val_acc: .2f}%\")\n",
        "        \n",
        "    save(net, 'net')\n",
        "    save(history, 'history')\n",
        "else:\n",
        "    net.load_state_dict(load('net'))\n",
        "    history = load('history')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwg7D6sv1kFL"
      },
      "source": [
        "### Visualize Loss and Accuracy plot\n",
        "\n",
        "Using the arrays that you have (from task 2 in the above section), visualize two plots: Accuracy plot (train and validation together) and Loss plot (train and validation together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zoTWbAVUbJw_",
        "outputId": "ac711806-85b0-4e77-c778-d97f965d50ee"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Train/Val Loss')\n",
        "plt.plot(history['train_loss'], '-o', label='train')\n",
        "plt.plot(history['val_loss'], '-o', label='val')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='lower left')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title('Train/Val Accuracy')\n",
        "plt.plot(history['train_acc'], '-o', label='train')\n",
        "plt.plot(history['val_acc'], '-o', label='val')\n",
        "plt.plot([80] * len(history['train_acc']), 'k--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('accuracy(%)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.gcf().set_size_inches(15, 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF_oCC3p2Q3C"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Test your trained model (using the Test Dataloader that you have). Our goal is to reach an accuracy above `80%`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzkNN4sMb3lK",
        "outputId": "28c07e06-f4c9-4e63-9405-89a1846c3f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss: 0.7574; test acc: 74.38%;\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc, incorrects = eval_epoch(net, criterion, testloader, test_mode=True)\n",
        "print(f\"train loss:{test_loss: .4f}; test acc:{test_acc: .2f}%;\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCKR1Xac2keH"
      },
      "source": [
        "## Visualize incorrectly predicted samples from testset\n",
        "\n",
        "Visualize *24* random images from testset that are incorrectly predicted by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew2KUBIWcG35",
        "outputId": "4c5cc746-5194-4cb7-b42d-a9abf4df59df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "45000"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJdFFFGL9T7L"
      },
      "source": [
        "## Exploring the feature space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTQ0aX0j9c7q"
      },
      "source": [
        "### Calculate the feature space for all training samples\n",
        "\n",
        "You have trained and evaluated your model. Now, for each sample in the trainset, calculate it's \"feature space\" discussed in the model section. The result of this section should be a tensor of size `(50000, N)` saved in a variable (for later usage)\n",
        "\n",
        "- **Hint 1:** define a tensor with dimension `(50000, N)` where *50000* is the size of the trainset and *N* is the dimension of the feature space\n",
        "\n",
        "- **Hint 2:** Pay attension to the `shuffle` attribute of your train dataloader (If needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbMzEiuqyP20"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDoLqddo-WJV"
      },
      "source": [
        "### K Nearest Neighbor in feature space\n",
        "\n",
        "You already have calculated the feature spaces for trainset ($S$) in the previous section\n",
        "\n",
        "1. Get 5 random samples from testset which are correctly predicted by the model.\n",
        "2. for each sample, calculate it's \"feature space\" ($X$)\n",
        "3. for each sample, calculate it's *5* nearest neighbors in \"feature space\" in the trainset (by comparing $X$ to each row in $S$) and visualize them\n",
        "\n",
        "\n",
        "**Hint:** For finding the nearest neighbors in the feature space you can use `torch.linalg.norm` and `torch.topk`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IBf3Hbpb6mZ"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X9aAFY3_g3w"
      },
      "source": [
        "### TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3DC15RI_loC"
      },
      "source": [
        "1. Sample $M$ ($2000$ would be enought) random samples from the trainset feature space (calculated in the above sections)\n",
        "2. Now you have a vector of size `(M, N)` where $N$ is the dimension of the feature space\n",
        "3. Using TSNE reduce $N$ to $2$ (Now you have a vector of size `(M, 2)`)\n",
        "4. Visualize the points in a 2D plane (Set color of each point based on it's class)\n",
        "\n",
        "**Hint:** You can use `sklearn.manifold.TSNE`\n",
        "\n",
        "**Hint:** Use `plt.scatter(x, y, c=labels)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsj2zTP6XeLX"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2NHVvUwF0-6"
      },
      "source": [
        "# CIFAR10 Colorization\n",
        "\n",
        "In this part of the assignment, we want to do an image colorization task using PyTorch on CIFAR10 dataset. We want to train a model that colors  a black-and-white image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-HuWeIBGlj1"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2-QrdeQGlj2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JCj_e-J11kr_",
        "outputId": "9b874eb5-4aa9-499a-dfe9-47c6dc63c613"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vljvz3SpG_Hs"
      },
      "source": [
        "## Custom Dataset\n",
        "\n",
        "Define a custom dataset class by extensing `torch.utils.data.Dataset`\n",
        "\n",
        "**Notice:** your dataset should output two things: black-and-white image and the RGB image\n",
        "\n",
        "**Hint:** You don't have to reinvent the wheel. Your class should just be a wrapper for CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUfsSe7ZHAkv"
      },
      "outputs": [],
      "source": [
        "class BlackAndWhiteCIFAR10(Dataset):\n",
        "    \"\"\"\n",
        "    Define a custom dataset class by extending `torch.utils.data.Dataset`\n",
        "    this class is a dataset for the CIFAR10 data in pytorch and it has the black and white image of the original CIFAR10 image as the data\n",
        "    and the original RGB image as the target\n",
        "    this class is just a wrapper for the torchvision.datasets.CIFAR10 class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train=True, root='./data', download=True, transform=None):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqoRjKPAG1y2"
      },
      "source": [
        "## Transforms & Dataset & Dataloader\n",
        "\n",
        "**Notice:** Use your defined custom dataset class for defining the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obw3INKm1XJg"
      },
      "outputs": [],
      "source": [
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUyYOcG_G2_P"
      },
      "outputs": [],
      "source": [
        "transform_train =\n",
        "\n",
        "transform_test ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "84d3ef1673a549fa81117d5487a344e2",
            "50db6971d00842b4927f0179cde2e0f7",
            "3097c1869fb045b58cdfdf5d853c329d",
            "ebbb00f6f62a4a648baa9ea9c636e292",
            "c367441d96514be989291b3dd1c190c7",
            "00c651315a3543f3aa9268414c21f684",
            "743735a1426048aebbec54ef6ed46214",
            "3b06cce1a4494758b530b27c9a8ec233",
            "091df5377aa94c709d901bc836719bbc",
            "91db8ce9d6a14c53a855fb015d07d536",
            "a06813e829404414be23fcd3c8d24000"
          ]
        },
        "id": "WlvY-A0R1uow",
        "outputId": "045b3bcd-5cff-4c2c-cef8-988cf96d187e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84d3ef1673a549fa81117d5487a344e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "initial_trainset = BlackAndWhiteCIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPP5qwLo18hG"
      },
      "outputs": [],
      "source": [
        "trainset, valset ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkYNF8ah19LX"
      },
      "outputs": [],
      "source": [
        "trainloader =\n",
        "valloader ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Nz6IWxINoq"
      },
      "source": [
        "## Dataset Visualization\n",
        "\n",
        "Visualize your dataset (black-and-white image along with the RGB image) by sampling from your trainset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjj-B6SSIPTU"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYXibse_I6Sw"
      },
      "source": [
        "## Model\n",
        "\n",
        "Define your model here (Input: black-and-white image, Output: RGB image)\n",
        "\n",
        "**Hint:** You can implement an autoencoder that does the colorization task for you. UNet could be a viable option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrAaIwPYI5Lp"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "class TransConvBlock(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(TransConvBlock, self).__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        pass\n",
        "\n",
        "class colorizationNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox3GdhWkKSfy"
      },
      "source": [
        "## Train\n",
        "\n",
        "Train your model\n",
        "\n",
        "Tasks:\n",
        "- [ ] Things that are needed to be printed in each epoch:\n",
        "  - Number of epoch\n",
        "  - Train loss\n",
        "  - Validation loss\n",
        "- [ ] save train/validation loss (of each epoch) in an array for later usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLH9z2zjKjEH"
      },
      "outputs": [],
      "source": [
        "net ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Q3WylJnrpc"
      },
      "outputs": [],
      "source": [
        "criterion =\n",
        "optimizer ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrnVWkAypq3-"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTvVBSOtqIX8"
      },
      "outputs": [],
      "source": [
        "def train_epoch(net: torch.nn.Module, criterion: torch.nn.Module, optimizer: torch.optim.Optimizer ,dataloader: torch.utils.data.DataLoader):\n",
        "    pass\n",
        "\n",
        "def eval_epoch(net: torch.nn.Module, criterion: torch.nn.Module, dataloader: torch.utils.data.DataLoader, test_mode: bool = False):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOoWOGuGqNbZ"
      },
      "outputs": [],
      "source": [
        "epochs =\n",
        "\n",
        "for e in range(epochs):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEM_ntRJLjLR"
      },
      "source": [
        "### Visualize Loss plot\n",
        "\n",
        "Using the arrays that you have (from task 2 in the above section), visualize the loss plot (train and validation together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pesG4263qeU2"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekWfxMkpKot4"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "1. Sample 36 random samples from testset (your own dataset class)\n",
        "2. Give each of the 36 samples to your trained model and get the outputs\n",
        "3. Visualize `input` (black-and-white image), `output` (output of the model with the given black-and-white input image) and `ground truth` (the actual RGB image)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8lhDXtgvqQl",
        "outputId": "6240684b-f76e-4efd-f51f-8f304aeace09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "testset =\n",
        "testloader ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9BjzM16yZgg"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "interpreter": {
      "hash": "dff97b9f14a22ccae10e7a517c30d03fcee05a8617da6e3ca20a923077f5eb08"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00c651315a3543f3aa9268414c21f684": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091df5377aa94c709d901bc836719bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3097c1869fb045b58cdfdf5d853c329d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b06cce1a4494758b530b27c9a8ec233",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_091df5377aa94c709d901bc836719bbc",
            "value": 170498071
          }
        },
        "3b06cce1a4494758b530b27c9a8ec233": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50db6971d00842b4927f0179cde2e0f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00c651315a3543f3aa9268414c21f684",
            "placeholder": "​",
            "style": "IPY_MODEL_743735a1426048aebbec54ef6ed46214",
            "value": "100%"
          }
        },
        "743735a1426048aebbec54ef6ed46214": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84d3ef1673a549fa81117d5487a344e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50db6971d00842b4927f0179cde2e0f7",
              "IPY_MODEL_3097c1869fb045b58cdfdf5d853c329d",
              "IPY_MODEL_ebbb00f6f62a4a648baa9ea9c636e292"
            ],
            "layout": "IPY_MODEL_c367441d96514be989291b3dd1c190c7"
          }
        },
        "91db8ce9d6a14c53a855fb015d07d536": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a06813e829404414be23fcd3c8d24000": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c367441d96514be989291b3dd1c190c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebbb00f6f62a4a648baa9ea9c636e292": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91db8ce9d6a14c53a855fb015d07d536",
            "placeholder": "​",
            "style": "IPY_MODEL_a06813e829404414be23fcd3c8d24000",
            "value": " 170498071/170498071 [00:01&lt;00:00, 87093789.07it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
