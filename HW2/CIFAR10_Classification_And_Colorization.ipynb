{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAa6g3BcFCOi"
      },
      "source": [
        "# CIFAR10 Classification\n",
        "\n",
        "### HW2 @ DL Course, Dr. Soleymani\n",
        "\n",
        "*Full Name:* Mehdi Jamalkhah\n",
        "\n",
        "*SID:* ...\n",
        "\n",
        "In this part of the assignment we want to do an image classification task using PyTorch on CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej8Q1gQPbTyE"
      },
      "source": [
        "### Google Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8kheJRyVeFZ",
        "outputId": "18b1a9ea-faf2-4bed-d462-85e3a30e7f9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srq7CBcGVl18",
        "outputId": "2f923710-ae18-4e84-95b7-f0a03204c85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['data', 'CIFAR10_Classification_And_Colorization.ipynb', 'DL']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'DL/HW2'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeGWCdkYbQCo"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "os.chdir(GOOGLE_DRIVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GsrCzWTOIhI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIv1Mpsbvt8v"
      },
      "source": [
        "## Device\n",
        "\n",
        "Set device to work with (GPU or CPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2uGuIUtwSFAR",
        "outputId": "9eb4dcab-a0e7-4900-d49a-6b0cae061725"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4T4AL0cv1Jm"
      },
      "source": [
        "## Transforms & Dataset & Dataloader\n",
        "\n",
        "Here, you should download and load the dataset with the desire transforms. After that, you should split train dataset to train and validation sets. Finally, define the dataloaders for `train`, `validation` and `test`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E6TT8whO9N4"
      },
      "outputs": [],
      "source": [
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuo6wyCkEqMK"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR0BpY0YO-Em"
      },
      "outputs": [],
      "source": [
        "# inverse the normilize transform to restore the original data\n",
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        tensor = tensor * self.std + self.mean\n",
        "        return tensor\n",
        "\n",
        "norminv = UnNormalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3UuPXFQOSDX"
      },
      "outputs": [],
      "source": [
        "initial_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=False, transform=transform_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61n36ZUkD-gA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "trainset, valset = random_split(initial_trainset, [45000, 5000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXhlMgGkEBPs"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "godQEnIuEDRo",
        "outputId": "4120af6c-b97d-4326-9f0c-c62c7adbc19a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C-YjLZtwnq2"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Visualize 5 random images from each class in different columns\n",
        "\n",
        "- **Hint**:  You can use `plt.subplots` for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "sfwytktJVVCH",
        "outputId": "1099ef62-484a-496f-acf5-573c50e43563"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "indexes = np.random.randint(0 , len(images), 5)\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i, idx in enumerate(indexes):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(images[idx].permute(1, 2, 0))\n",
        "    plt.title(f\"Label: {classes[labels[idx]]}\")\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja-L0f1wybzU"
      },
      "source": [
        "## Model\n",
        "\n",
        "Define your ResNet model here from scratch (You are not allowed to use the existing models in pytorch)\n",
        "\n",
        "Our suggestion is to implement ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZtjLZl2VVCH"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ResNetBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, identity_downsample=None,stride=1):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUYoc5gVVkTP"
      },
      "outputs": [],
      "source": [
        "class ResNetLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1) -> None:\n",
        "        super(ResNetLayer, self).__init__()\n",
        "\n",
        "        identity_downsample = None\n",
        "        if stride != 1:\n",
        "            identity_downsample =  nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.block1 = ResNetBlock(in_channels, out_channels, identity_downsample, stride=stride)\n",
        "        self.block2 = ResNetBlock(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UxdLeNUVVCH"
      },
      "outputs": [],
      "source": [
        "class ResNet18(torch.nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = ResNetLayer(64, 64)\n",
        "        self.layer2 = ResNetLayer(64, 128, stride=2)\n",
        "        self.layer3 = ResNetLayer(128, 256, stride=2)\n",
        "        self.layer4 = ResNetLayer(256, 512, stride=2)\n",
        "\n",
        "        self.avgPool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgPool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYp77Euaz_5u"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odR5mfCA0Eqy"
      },
      "source": [
        "### Model instantiation\n",
        "\n",
        "Create an instance of your model and move it to `device`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3PjKY_oSBkg"
      },
      "outputs": [],
      "source": [
        "net = ResNet18().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8zn5eLs0bBS"
      },
      "source": [
        "### Criterion & Optimizater\n",
        "\n",
        "Define `criterion` and `optimizer` (Or `scheduler`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEd5yXt3SL2T"
      },
      "outputs": [],
      "source": [
        "criterion =  nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.002, weight_decay=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gth9e1k70uAI"
      },
      "source": [
        "### Train loop\n",
        "\n",
        "Train your model\n",
        "\n",
        "Tasks:\n",
        "- [ ] Things that are needed to be printed in each epoch:\n",
        "  - Number of epoch\n",
        "  - Train loss\n",
        "  - Train accuracy\n",
        "  - Validation loss\n",
        "  - Validation accuracy\n",
        "- [ ] save train/validation loss and accuracy (of each epoch) in an array for later usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONOcKjTxpDpQ"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y87a3RWjVVCJ"
      },
      "outputs": [],
      "source": [
        "def correct(output, target):\n",
        "    _, predicted = output.max(1)\n",
        "    return predicted.eq(target).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFz0lVkUs8Mk"
      },
      "outputs": [],
      "source": [
        "def train_epoch(net: torch.nn.Module, criterion: torch.nn.Module, optimizer: torch.optim.Optimizer,scheduler: torch.optim.lr_scheduler ,dataloader: torch.utils.data.DataLoader):\n",
        "    net.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch, labels in dataloader:\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = net(batch)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total += labels.size(0)\n",
        "        corrects += correct(output, labels)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100 * corrects / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def eval_epoch(net: torch.nn.Module, criterion: torch.nn.Module, dataloader: torch.utils.data.DataLoader, test_mode: bool = False):\n",
        "    net.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    if test_mode:\n",
        "      incorrect_images = torch.tensor([]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, labels in dataloader:\n",
        "            batch, labels = batch.to(device), labels.to(device)\n",
        "\n",
        "            outputs = net(batch)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            corrects += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if test_mode:\n",
        "              incorrect_images = torch.concatenate([incorrect_images, batch[predicted.eq(labels)]])\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100.0 * corrects / total\n",
        "\n",
        "    if test_mode:\n",
        "      return avg_loss, accuracy, incorrect_images\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Hm6VdzjEQA"
      },
      "outputs": [],
      "source": [
        "epochs = 8\n",
        "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]}\n",
        "for e in range(epochs):\n",
        "    train_loss, train_acc = train_epoch(net, criterion, optimizer, scheduler, trainloader)\n",
        "    val_loss, val_acc = eval_epoch(net, criterion, valloader)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"(Epoch {e + 1} / {epochs}) train loss:{train_loss: .4f}; train acc:{train_acc: .2f}%; val loss:{val_loss: .4f}; val_acc:{val_acc: .2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwg7D6sv1kFL"
      },
      "source": [
        "### Visualize Loss and Accuracy plot\n",
        "\n",
        "Using the arrays that you have (from task 2 in the above section), visualize two plots: Accuracy plot (train and validation together) and Loss plot (train and validation together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoTWbAVUbJw_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Train/Val Loss')\n",
        "plt.plot(history['train_loss'], '-o', label='train')\n",
        "plt.plot(history['val_loss'], '-o', label='val')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='lower left')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title('Train/Val Accuracy')\n",
        "plt.plot(history['train_acc'], '-o', label='train')\n",
        "plt.plot(history['val_acc'], '-o', label='val')\n",
        "plt.plot([80] * len(history['train_acc']), 'k--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('accuracy(%)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.gcf().set_size_inches(15, 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF_oCC3p2Q3C"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Test your trained model (using the Test Dataloader that you have). Our goal is to reach an accuracy above `80%`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzkNN4sMb3lK",
        "outputId": "28c07e06-f4c9-4e63-9405-89a1846c3f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss: 2.3024; test acc: 9.34%;\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc, incorrects = eval_epoch(net, criterion, testloader, test_mode=True)\n",
        "print(f\"train loss:{test_loss: .4f}; test acc:{test_acc: .2f}%;\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCKR1Xac2keH"
      },
      "source": [
        "## Visualize incorrectly predicted samples from testset\n",
        "\n",
        "Visualize *24* random images from testset that are incorrectly predicted by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew2KUBIWcG35",
        "outputId": "4c5cc746-5194-4cb7-b42d-a9abf4df59df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "45000"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJdFFFGL9T7L"
      },
      "source": [
        "## Exploring the feature space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTQ0aX0j9c7q"
      },
      "source": [
        "### Calculate the feature space for all training samples\n",
        "\n",
        "You have trained and evaluated your model. Now, for each sample in the trainset, calculate it's \"feature space\" discussed in the model section. The result of this section should be a tensor of size `(50000, N)` saved in a variable (for later usage)\n",
        "\n",
        "- **Hint 1:** define a tensor with dimension `(50000, N)` where *50000* is the size of the trainset and *N* is the dimension of the feature space\n",
        "\n",
        "- **Hint 2:** Pay attension to the `shuffle` attribute of your train dataloader (If needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbMzEiuqyP20"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDoLqddo-WJV"
      },
      "source": [
        "### K Nearest Neighbor in feature space\n",
        "\n",
        "You already have calculated the feature spaces for trainset ($S$) in the previous section\n",
        "\n",
        "1. Get 5 random samples from testset which are correctly predicted by the model.\n",
        "2. for each sample, calculate it's \"feature space\" ($X$)\n",
        "3. for each sample, calculate it's *5* nearest neighbors in \"feature space\" in the trainset (by comparing $X$ to each row in $S$) and visualize them\n",
        "\n",
        "\n",
        "**Hint:** For finding the nearest neighbors in the feature space you can use `torch.linalg.norm` and `torch.topk`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IBf3Hbpb6mZ"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X9aAFY3_g3w"
      },
      "source": [
        "### TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3DC15RI_loC"
      },
      "source": [
        "1. Sample $M$ ($2000$ would be enought) random samples from the trainset feature space (calculated in the above sections)\n",
        "2. Now you have a vector of size `(M, N)` where $N$ is the dimension of the feature space\n",
        "3. Using TSNE reduce $N$ to $2$ (Now you have a vector of size `(M, 2)`)\n",
        "4. Visualize the points in a 2D plane (Set color of each point based on it's class)\n",
        "\n",
        "**Hint:** You can use `sklearn.manifold.TSNE`\n",
        "\n",
        "**Hint:** Use `plt.scatter(x, y, c=labels)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsj2zTP6XeLX"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2NHVvUwF0-6"
      },
      "source": [
        "# CIFAR10 Colorization\n",
        "\n",
        "In this part of the assignment, we want to do an image colorization task using PyTorch on CIFAR10 dataset. We want to train a model that colors  a black-and-white image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-HuWeIBGlj1"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2-QrdeQGlj2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JCj_e-J11kr_",
        "outputId": "9b874eb5-4aa9-499a-dfe9-47c6dc63c613"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vljvz3SpG_Hs"
      },
      "source": [
        "## Custom Dataset\n",
        "\n",
        "Define a custom dataset class by extensing `torch.utils.data.Dataset`\n",
        "\n",
        "**Notice:** your dataset should output two things: black-and-white image and the RGB image\n",
        "\n",
        "**Hint:** You don't have to reinvent the wheel. Your class should just be a wrapper for CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUfsSe7ZHAkv"
      },
      "outputs": [],
      "source": [
        "class BlackAndWhiteCIFAR10(Dataset):\n",
        "    \"\"\"\n",
        "    Define a custom dataset class by extending `torch.utils.data.Dataset`\n",
        "    this class is a dataset for the CIFAR10 data in pytorch and it has the black and white image of the original CIFAR10 image as the data\n",
        "    and the original RGB image as the target\n",
        "    this class is just a wrapper for the torchvision.datasets.CIFAR10 class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train=True, root='./data', download=True, transform=None):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqoRjKPAG1y2"
      },
      "source": [
        "## Transforms & Dataset & Dataloader\n",
        "\n",
        "**Notice:** Use your defined custom dataset class for defining the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obw3INKm1XJg"
      },
      "outputs": [],
      "source": [
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUyYOcG_G2_P"
      },
      "outputs": [],
      "source": [
        "transform_train =\n",
        "\n",
        "transform_test ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "84d3ef1673a549fa81117d5487a344e2",
            "50db6971d00842b4927f0179cde2e0f7",
            "3097c1869fb045b58cdfdf5d853c329d",
            "ebbb00f6f62a4a648baa9ea9c636e292",
            "c367441d96514be989291b3dd1c190c7",
            "00c651315a3543f3aa9268414c21f684",
            "743735a1426048aebbec54ef6ed46214",
            "3b06cce1a4494758b530b27c9a8ec233",
            "091df5377aa94c709d901bc836719bbc",
            "91db8ce9d6a14c53a855fb015d07d536",
            "a06813e829404414be23fcd3c8d24000"
          ]
        },
        "id": "WlvY-A0R1uow",
        "outputId": "045b3bcd-5cff-4c2c-cef8-988cf96d187e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84d3ef1673a549fa81117d5487a344e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "initial_trainset = BlackAndWhiteCIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPP5qwLo18hG"
      },
      "outputs": [],
      "source": [
        "trainset, valset ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkYNF8ah19LX"
      },
      "outputs": [],
      "source": [
        "trainloader =\n",
        "valloader ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Nz6IWxINoq"
      },
      "source": [
        "## Dataset Visualization\n",
        "\n",
        "Visualize your dataset (black-and-white image along with the RGB image) by sampling from your trainset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjj-B6SSIPTU"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYXibse_I6Sw"
      },
      "source": [
        "## Model\n",
        "\n",
        "Define your model here (Input: black-and-white image, Output: RGB image)\n",
        "\n",
        "**Hint:** You can implement an autoencoder that does the colorization task for you. UNet could be a viable option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrAaIwPYI5Lp"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "class TransConvBlock(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(TransConvBlock, self).__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        pass\n",
        "\n",
        "class colorizationNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox3GdhWkKSfy"
      },
      "source": [
        "## Train\n",
        "\n",
        "Train your model\n",
        "\n",
        "Tasks:\n",
        "- [ ] Things that are needed to be printed in each epoch:\n",
        "  - Number of epoch\n",
        "  - Train loss\n",
        "  - Validation loss\n",
        "- [ ] save train/validation loss (of each epoch) in an array for later usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLH9z2zjKjEH"
      },
      "outputs": [],
      "source": [
        "net ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Q3WylJnrpc"
      },
      "outputs": [],
      "source": [
        "criterion =\n",
        "optimizer ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrnVWkAypq3-"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTvVBSOtqIX8"
      },
      "outputs": [],
      "source": [
        "def train_epoch(net: torch.nn.Module, criterion: torch.nn.Module, optimizer: torch.optim.Optimizer ,dataloader: torch.utils.data.DataLoader):\n",
        "    pass\n",
        "\n",
        "def eval_epoch(net: torch.nn.Module, criterion: torch.nn.Module, dataloader: torch.utils.data.DataLoader, test_mode: bool = False):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOoWOGuGqNbZ"
      },
      "outputs": [],
      "source": [
        "epochs =\n",
        "\n",
        "for e in range(epochs):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEM_ntRJLjLR"
      },
      "source": [
        "### Visualize Loss plot\n",
        "\n",
        "Using the arrays that you have (from task 2 in the above section), visualize the loss plot (train and validation together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pesG4263qeU2"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekWfxMkpKot4"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "1. Sample 36 random samples from testset (your own dataset class)\n",
        "2. Give each of the 36 samples to your trained model and get the outputs\n",
        "3. Visualize `input` (black-and-white image), `output` (output of the model with the given black-and-white input image) and `ground truth` (the actual RGB image)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8lhDXtgvqQl",
        "outputId": "6240684b-f76e-4efd-f51f-8f304aeace09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "testset =\n",
        "testloader ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9BjzM16yZgg"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "interpreter": {
      "hash": "dff97b9f14a22ccae10e7a517c30d03fcee05a8617da6e3ca20a923077f5eb08"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00c651315a3543f3aa9268414c21f684": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091df5377aa94c709d901bc836719bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3097c1869fb045b58cdfdf5d853c329d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b06cce1a4494758b530b27c9a8ec233",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_091df5377aa94c709d901bc836719bbc",
            "value": 170498071
          }
        },
        "3b06cce1a4494758b530b27c9a8ec233": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50db6971d00842b4927f0179cde2e0f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00c651315a3543f3aa9268414c21f684",
            "placeholder": "​",
            "style": "IPY_MODEL_743735a1426048aebbec54ef6ed46214",
            "value": "100%"
          }
        },
        "743735a1426048aebbec54ef6ed46214": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84d3ef1673a549fa81117d5487a344e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50db6971d00842b4927f0179cde2e0f7",
              "IPY_MODEL_3097c1869fb045b58cdfdf5d853c329d",
              "IPY_MODEL_ebbb00f6f62a4a648baa9ea9c636e292"
            ],
            "layout": "IPY_MODEL_c367441d96514be989291b3dd1c190c7"
          }
        },
        "91db8ce9d6a14c53a855fb015d07d536": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a06813e829404414be23fcd3c8d24000": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c367441d96514be989291b3dd1c190c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebbb00f6f62a4a648baa9ea9c636e292": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91db8ce9d6a14c53a855fb015d07d536",
            "placeholder": "​",
            "style": "IPY_MODEL_a06813e829404414be23fcd3c8d24000",
            "value": " 170498071/170498071 [00:01&lt;00:00, 87093789.07it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}